
---

## ğŸ‡ªğŸ‡¸ docs/README.es.md
```markdown
# ğŸš€ OQS_step2 â€” API Flask Dockerizada (Ollama + OpenAI)

Una **API de chat en Flask Dockerizada** con soporte para **Ollama (local, gratis)** y **OpenAI API (remoto, de pago)**.  
DiseÃ±ada para ejecutarse en cualquier entorno y servir como base para integraciones de IA en portafolio o proyectos de producciÃ³n.

---

## âœ¨ CaracterÃ­sticas
- ğŸŒ API REST en **Flask**
- ğŸ“¦ Deploy con **Docker + Compose**
- ğŸ”€ Soporte multiproveedor:
  - **Ollama** â†’ modelos locales (`llama3.2:3b`, etc.)
  - **OpenAI** â†’ GPT-4o, GPT-4o-mini, etc.
- âš¡ Endpoints compatibles con **OpenAI API**
- ğŸ›¡ï¸ Configurable vÃ­a `.env`
- ğŸ” Healthcheck y estado incorporados

---

## ğŸ› ï¸ Quickstart

### 1. Clonar repositorio
```bash
git clone https://github.com/TU-USUARIO/oqs_step2.git
cd oqs_step2

2. Crear .env
cp .env.example .env
"Edite para elegir PROVIDER=ollama (predeterminado) o openai."

3. Levantar con Docker Compose
docker compose up -d --build

4. Probar salud
curl http://localhost:8000/health

âš™ï¸ Endpoints principales
GET  /config
GET  /health
GET  /models
POST /chat               # { "message": "Â¡Hola, mundo!" }
POST /v1/chat/completions
GET  /v1/models

ğŸ§ª Pruebas rÃ¡pidas
Ollama (predeterminado)
curl -X POST http://localhost:8000/chat \
  -H "Content-Type: application/json" \
  -d '{"message": "Hola en 3 idiomas"}'

OpenAI (si tienes clave)
export PROVIDER=openai
export OPENAI_API_KEY=sk-xxxx

docker compose run --rm tester

ğŸ—‚ï¸ Estructura del Proyecto
.
â”œâ”€ app/
â”‚  â”œâ”€ __init__.py       # API principal Flask
â”‚  â”œâ”€ ollama_client.py  # Cliente Ollama
â”‚  â””â”€ static/           # Frontend (si existe)
â”œâ”€ test_ai.py           # Script de prueba (Ollama/OpenAI)
â”œâ”€ Dockerfile
â”œâ”€ docker-compose.yml
â”œâ”€ docker-compose.override.yml
â”œâ”€ .env.example
â”œâ”€ LICENSE
â””â”€ docs/
   â””â”€ LICENSE.md


ğŸ“– Licencia
Distribuido bajo la .
Explicaciones multilingÃ¼es en .

